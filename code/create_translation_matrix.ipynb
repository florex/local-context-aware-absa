{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"create_translation_matrix.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNHgYkWGmKhTsavcqNwVfOc"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"blXzRonrrOO6"},"source":["#pip install tensorflow==1.13.1\n","#pip install keras==2.2.4\n","!pip install emoji\n","!pip install fasttext\n","!pip install unidecode\n","!pip install gensim==3.8.3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2dheGg2Yq_i0","executionInfo":{"status":"ok","timestamp":1607459395592,"user_tz":-60,"elapsed":2468,"user":{"displayName":"Franklin Dongmo nzoiyem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi7AvwmfyGzh2fAZxifqsrWYdRNaic02XVH0YZ4=s64","userId":"14051344547408147470"}},"outputId":"97ee85e8-73bb-4148-8f77-2859532ce300"},"source":["import numpy as np\n","import emoji\n","import re\n","import string\n","import fasttext\n","import fasttext.util\n","from gensim.models.wrappers import FastText\n","from gensim.models import KeyedVectors, TranslationMatrix\n","from gensim.models.wrappers import FastText\n","\n","\n","import keras\n","from keras.utils import to_categorical\n","import numpy as np\n","import pickle\n","\n","from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n","from keras.preprocessing.sequence import pad_sequences\n","from xml.dom import minidom\n","from string import punctuation\n","import sys,string\n","class Prepocessing:\n","    def __init__(self,with_gensim=True,path_to_data=\"\"):\n","        \"\"\"@path_to_data represent the dict where are store pickle object who \n","            contain data, sentences, tokens\"\"\"\n","        self.my_punctuation=string.punctuation+\"»«'´’…‘”“,¨¦´ʼʽʾʿˀˁ͵ͺ΄ʻ·\"\n","        self.data=[]\n","        self.sentences=[]\n","        self.all_sentences=[]\n","        self.test_sentences=[]\n","        self.embedding={}\n","        self.tokenizer={}\n","        self.rever_tokenizer={}\n","        self.vocab_size=0\n","        self.max_length=0\n","        self.word2vec={}\n","        if(path_to_data!=\"\"):\n","            with open(path_to_data,\"rb\") as f:\n","                temp=pickle.loads(f.read())\n","                self.data=temp[\"data\"]\n","                self.sentences=temp[\"sentences\"]\n","                self.vocab=temp[\"tokens\"]\n","    \n","    def clean_emoji(self,text):\n","      '''\n","        remove all of emojis from text\n","        -------------------------\n","      '''\n","      elt_2=self.replace_all_shorcuts(text)\n","      elt_2=self.replace_punctuaion(elt_2,string.punctuation+\"»«'´’…‘”“,¨¦´ʼʽʾʿˀˁ͵ͺ΄ʻ·\")\n","      text=  emoji.demojize(elt_2)\n","      text= re.sub(r'(:[!_\\-\\w]+:)', '', text)\n","      return text\n","    \n","    def load_txt_sentences(self,file_path):\n","      \"\"\"Charge les donnees d'un fichier texte sous forme de phrase\"\"\"\n","\n","      with open(file_path) as f:\n","        data=f.read()\n","        data=data.split(\"\\n\")\n","      return data          \n","        \n","#        if(with_gensim==True):\n","#            self.word2vec=gensim.models.KeyedVectors.load_word2vec_format(\"word_representation/google_300_wtv.bin\", binary=True)\n","#        else:\n","#            with open(\"word_representation/personnal_embedding.bin\",\"rb\") as f:\n","#                self.word2vec=pickle.loads(f.read())\n","            \n","\n","    def get_word_representation(self,word):\n","        try:\n","            array_rep=self.word2vec[word.lower()]\n","            return array_rep\n","        except:\n","            return np.random.uniform(-0.25, 0.25, 300)\n","    \n","    def load_xml_data_with_dom(self,path_to_data,test=False,with_punc=False):\n","        data=\"\"\n","        with open(path_to_data,\"r\") as f:\n","            data=f.read()\n","        parse_data=minidom.parseString(data)\n","        #ght=parse_data.getElementsByTagName(\"ds\")\n","        #print(ght.length)\n","        sentences=parse_data.getElementsByTagName(\"sentences\")[0].getElementsByTagName(\"sentence\")\n","        #ght=Element()\n","        #ght.TEXT_NODE\n","        #print(sentences[0].TEXT_NODE)\n","        i=0\n","        for sentence in sentences:\n","            nsentence={\"text\":\"\",\"aspects\":[]}\n","            text=sentence.getElementsByTagName(\"text\")[0]\n","            nsentence[\"text\"]=self.replace_all_shorcuts(text.firstChild.nodeValue.lower())\n","            if with_punc==True:\n","                nsentence[\"text\"]=self.replace_punctuaion(nsentence[\"text\"],punctuation)\n","                nsentence[\"text\"]=nsentence[\"text\"].replace(\"'\",\" \")\n","                nsentence[\"text\"]=\" \".join(nsentence[\"text\"].split())\n","            try:\n","                \n","                #now we find the aspecterms terms\n","                aspect_terms=sentence.getElementsByTagName(\"aspectTerms\")\n","                for apt in aspect_terms:\n","                    aspects=apt.getElementsByTagName(\"aspectTerm\")\n","                    for aspect in aspects:\n","                        n_aspect={\"polarity\":\"neutral\"}\n","                        n_aspect[\"term\"]=self.replace_all_shorcuts(aspect.attributes[\"term\"].value.lower())\n","                        n_aspect[\"term\"]=n_aspect[\"term\"].replace(\"'\",\" \")\n","                        n_aspect[\"term\"]=\" \".join(n_aspect[\"term\"].split())\n","                        n_aspect[\"from\"]=aspect.attributes[\"from\"].value\n","                        n_aspect[\"to\"]=aspect.attributes[\"to\"].value\n","                        if(test==False):\n","                            n_aspect[\"polarity\"]=aspect.attributes[\"polarity\"].value.lower()\n","                        nsentence[\"aspects\"].append(n_aspect)\n","                if(test==True):\n","                    self.test_sentences.append(nsentence)\n","                else:\n","                    self.sentences.append(nsentence)\n","                self.all_sentences.append(nsentence[\"text\"])\n","            except Exception as e:\n","                print(nsentence[\"text\"])\n","                print(i)\n","                print(e)\n","                break\n","                \n","            i=i+1\n","    \n","    def save_data_to_file(self,path):\n","        \"\"\"Save the data like a pickle object in a file\"\"\"\n","        \n","        dict_data={\"tokenizer\":self.tokenizer,\"sentences\":self.sentences,\"data\":self.data,\"max_length\":self.max_length,\"vocab_size\":self.vocab_size,\"all_sentences\":self.all_sentences,\"test_sentences\":self.test_sentences,\"rever_tokenizer\":self.rever_tokenizer}\n","        with open(path,\"wb\") as f:\n","            pickle.dump(dict_data,f)\n","    \n","    \n","    def save_tokenizer_information(self,path):\n","        \"\"\"Sauvegarde les informations sur le tokenizer\"\"\"\n","        dict_data={\"tokenizer\":self.tokenizer,\"vocab_size\":self.vocab_size,\"max_length\":self.max_length,\"rever_tokenizer\":self.rever_tokenizer}\n","        with open(path,\"wb\") as f:\n","            pickle.dump(dict_data,f)\n","    \n","    def load_tokenizer(self,path):\n","        with open(path,\"rb\") as f:\n","            dict_data=pickle.loads(f.read())\n","            self.tokenizer=dict_data[\"tokenizer\"]\n","            self.max_length=dict_data[\"max_length\"]\n","            self.vocab_size=dict_data[\"vocab_size\"]\n","            self.rever_tokenizer=dict_data.get(\"rever_tokenizer\",{})\n","    \n","    \n","    def load_data_from_file(self,path):\n","        \"\"\"Load picke object data from file\"\"\"\n","        with open(path,\"rb\") as f:\n","            dict_data=pickle.loads(f.read())\n","            self.tokenizer=dict_data[\"tokenizer\"]\n","            self.max_length=dict_data[\"max_length\"]\n","            self.vocab_size=dict_data[\"vocab_size\"]\n","            self.data=dict_data[\"data\"]\n","            self.sentences=dict_data[\"sentences\"]\n","            self.test_sentences=dict_data.get(\"test_sentences\",[])\n","            self.all_sentences=dict_data.get(\"all_sentences\",[])\n","            self.rever_tokenizer=dict_data.get(\"rever_tokenizer\",{})\n","    \n","    def replace_punctuaion(self,text,ponctuation):\n","        \"\"\"cette fonction cree de l'espace autour des ponctuations\"\"\" \n","        text=str(text)\n","        for elt in ponctuation:\n","            text=text.replace(elt,\" \"+elt+\" \")\n","        return text\n","    \n","    def remove_punctuation(self,text,ponctuation):\n","        \"\"\"Cette fonction supprimme la ponctuation \n","        passe en parametre dans le text\"\"\"\n","        text=str(text)\n","        for elt in ponctuation:\n","            text=text.replace(elt,\" \")\n","        return text\n","    def create_tokenizer(self,with_punc=False):\n","        \"\"\"A partirs de la liste des phrases on va creer un vocabulaire\n","        Dans un objet tokenizer de keras\n","        @with_punc permet de savoir si la ponctuation sera ignorer ou\"\"\"\n","        if with_punc==True:\n","            filters=\"\\t\\n\"\n","            tokenizer=Tokenizer(filters=filters)\n","        else:\n","            tokenizer=Tokenizer()\n","        tokenizer.fit_on_texts(self.all_sentences)\n","        self.vocab_size=len(tokenizer.word_index)+1\n","        self.tokenizer=tokenizer\n","        self.rever_tokenizer=dict(map(reversed, tokenizer.word_index.items()))\n","        return tokenizer\n","    \n","    def set_sentences_max_length(self):\n","        \"\"\"\n","        return and et the lenght of longeur sentence per word\n","        \"\"\"\n","        self.max_length=max([len(self.tokenizer.texts_to_sequences([sentence])[0]) for sentence in self.all_sentences])\n","        \n","        return self.max_length\n","    \n","    def encode_text_to_vect(self,texts):\n","        \"\"\"Use tokernizer to create a vector of entry text\n","            param @texts is a list of sentence only text\n","            return @sequence by using @self.max_length\n","        \"\"\"\n","        sequence=self.tokenizer.texts_to_sequences(texts)\n","        \n","        #print(sequence)\n","        sequence=pad_sequences(sequences=sequence,maxlen=self.max_length,padding=\"post\", truncating=\"post\")\n","        return sequence\n","    \n","    def get_aspect_position(self,text,aspect_term,from_, to_):\n","        \"\"\"cette fonction prends en entrer une phrase et deux indices et \n","           retourne les indices de cette deniere\"\"\"\n","        aspects=aspect_term.split(\" \")\n","        #print(aspects)\n","        text_token=text.split()\n","        aspects_position=[]\n","        j=0 #contient le debut de chaque mot en tant que indice dans la phrase\n","        i=0 #compteur dans la boucle\n","        start_index=0\n","        exist=False\n","        #print(from_)\n","        for wor in text_token:\n","            #print(j)\n","            if(j==from_ or j==(from_-1)):\n","                start_index=i\n","                exist=True\n","                break\n","            else:\n","                j=j+len(wor)+1\n","            i=i+1\n","        if exist==True:\n","            for aspect in aspects:\n","                aspects_position.append(start_index)\n","                start_index=start_index+1\n","        return aspects_position\n","    \n","    def get_aspect_position_2(self,text,aspect_term,begin):\n","        i=0\n","        j=0\n","        start_index=0\n","        exist=False\n","        aspects_index=[]\n","        ponct2={car:True for car in punctuation}\n","        ponct2[\" \"]=True\n","        #print(ponct2)\n","        for car in text:\n","            #print(car)\n","            test_car=ponct2.get(car,None)\n","            #print(test_car)\n","            if test_car:\n","                i=i+1\n","            if(j==begin):\n","                #print(i)\n","                #print(j,begin)\n","                start_index=i\n","                exist=True\n","            j=j+1\n","        if exist==True:\n","            for aspect in aspect_term.split():\n","                aspects_index.append(start_index)\n","                start_index=start_index+1\n","        return aspects_index\n","    \n","    def replace_all_shorcuts(self,text):\n","        \"\"\"\n","           cette fonction remplace les raccourcis d'usage utilise en anglais. Nous pensons que cela peut \n","           compresser de l'information et faire apprendre au model plus que necessaire\n","        \"\"\"\n","        shorcuts={\"'s\":\" is\",\"i've\":\"i have\",\"'ll\":\" will\",\"n't\":\" not\",\"'re\":\" are\",\"'d\":\" had\",\"'m\":\" am\",\"'ve\":\" have\"}\n","        text=text.lower()\n","        for elt in shorcuts:\n","            text=text.replace(elt,shorcuts[elt])\n","        return text\n","    \n","    def get_aspect_position_3(self,text,aspect_term,begin):\n","        \"\"\"Cette fonction retourne la position d'un aspect en tant que mot \n","        dans une liste de mot\"\"\"\n","        \n","        text_tokens=text_to_word_sequence(text.lower(), filters=self.tokenizer.filters)\n","        aspect_tokens=text_to_word_sequence(aspect_term.lower(), filters=self.tokenizer.filters)\n","        new_text=\" \".join(text_tokens)\n","        new_aspect=\" \".join(aspect_tokens)\n","        new_aspect=new_aspect.replace(\"'\",\"\")\n","        new_text=new_text.replace(\"'\",\"\")\n","        \n","        #all_occure=re.finditer(new_aspect,new_text)\n","        list_index=find_list_in_list(aspect_tokens,text_tokens)\n","        try:\n","            index_0=list_index[0]\n","        except Exception as e:\n","            try:\n","                new_aspect=self.replace_punctuaion(aspect_term,string.punctuation)\n","                new_text=new_text.replace(\"'\",\"\")\n","                text_tokens=text_to_word_sequence(new_text,filters=self.tokenizer.filters)\n","                aspect_tokens=text_to_word_sequence(new_aspect, filters=self.tokenizer.filters)\n","                \n","                list_index=find_list_in_list(aspect_tokens,text_tokens)\n","                \n","                index_0=list_index[0]\n","            except Exception as f:\n","                new_aspect=self.replace_punctuaion(aspect_term,string.punctuation)\n","#                new_text=new_text.replace(\"'\",\"\")\n","                text_tokens=text_to_word_sequence(new_text,filters=self.tokenizer.filters)\n","                aspect_tokens=text_to_word_sequence(new_aspect, filters=self.tokenizer.filters)\n","                print(\" \".join(text_tokens))\n","                print(\"\\n\",text,\"\\n\")\n","                print(\"\\n --- \\n\")\n","                print(\" \".join(aspect_tokens))\n","                print(\"\\n\\n\")\n","                sys.exit()\n","        #print(len(list_index))\n","        if len(list_index)>1:\n","            \n","            #on retrouve l'index le plus proche de begin\n","            begin_distances=[]\n","            for elt in list_index:\n","                length=len(\" \".join(text_tokens[0:elt+1]))\n","                begin_dist=abs(begin-length)\n","                begin_distances.append(begin_dist)\n","            min_dist_index=begin_distances.index(min(begin_distances))\n","            index_0=list_index[min_dist_index]\n","        return list(range(index_0,index_0+len(aspect_tokens)))    \n","        \n","        \n","        \n","        \n","    def get_ibo_vector(self,sentence,with_punc=False):\n","        \"\"\"take sentence in parameter which is dict struture like that\n","            {\"text\":text,\"aspects\":[{\"term\":term,\"from\":begin_index,\"to\":end_index},...]}\n","            and return the ibo vector of this sentence\n","        \"\"\"\n","        ibo_vector=np.zeros(self.max_length, dtype=\"int\")\n","        #print(sentence[\"text\"])\n","        aspect_index=[]\n","        for aspect in sentence[\"aspects\"]:\n","            #print(aspect[\"term\"])\n","            aspect_index=self.get_aspect_position_3(sentence[\"text\"],aspect[\"term\"],int(aspect[\"from\"]))\n","            i=0\n","            for index in aspect_index:\n","                if(i==0):\n","                    ibo_vector[index]=1.0\n","                else:\n","                    ibo_vector[index]=2.0\n","                i=i+1\n","        if with_punc==True:\n","            text_tokens=text_to_word_sequence(sentence[\"text\"],filters=self.tokenizer.filters)\n","            j=0\n","            for elt in text_tokens:\n","                if elt in string.punctuation and elt!=\"-\":\n","                    ibo_vector[j]=3.0\n","                j=j+1\n","        \n","        return ibo_vector\n","    \n","            \n","    def create_train_data(self,test=False,nb_classe=3,with_punc=False):\n","        \"\"\"Cette fonction va mettre toute les phrases sous forme de sequence \n","            et faire un one hot encoding des mots\"\"\"\n","        train_data_x=[]\n","        train_data_y=[]\n","        real_ibo=[]\n","        if(test==False):\n","            sentences=[sentence[\"text\"] for sentence in self.sentences]\n","        else:\n","            sentences=[sentence[\"text\"] for sentence in self.test_sentences]\n","        for sentence in self.sentences:\n","            ibo=self.get_ibo_vector(sentence,with_punc)\n","            real_ibo.append(ibo)\n","            train_data_y.append(to_categorical(ibo,nb_classe))\n","        #print(sentences)\n","        train_data_x=np.array(self.encode_text_to_vect(sentences))\n","        #train_x=[]\n","        #for x in train_data_x:\n","        #    train_x.append(to_categorical(x,self.vocab_size))\n","        #train_data_x=np.array(train_x)\n","        return {\"train_x\":train_data_x,\"train_y\":np.array(train_data_y),\"real_ibo\":real_ibo}\n","    def save_object(self,object_to_save,path):\n","        with open(path,\"wb\") as f:\n","            pickle.dump(object_to_save,f)\n","    \n","    def load_object(self,path):\n","        with open(path,\"rb\") as f:\n","            return pickle.loads(f.read())\n","    \n","    def create_test_data(self,with_punc=False):\n","        test_data_x=[]\n","        test_data_y=[]\n","        sentences=[sentence[\"text\"] for sentence in self.test_sentences]\n","        i=0\n","        for sentence in self.test_sentences:\n","            try:\n","                test_data_y.append(self.get_ibo_vector(sentence,with_punc))\n","            except Exception as e:\n","                print(sentence)\n","                print(i)\n","            i=i+1\n","        test_data_x=np.array(self.encode_text_to_vect(sentences))\n","        return {\"test_x\":test_data_x,\"test_y\":np.array(test_data_y)}\n","     \n","    def save_test_data(self,data,path):\n","        with open(path,\"wb\") as f:\n","            pickle.dump(data,f)\n","    \n","    def load_test_data(self,path):\n","        with open(path,\"rb\") as f:\n","            data=pickle.loads(f.read())\n","        return data\n","    def get_aspects_term(self,sentence):\n","        pass\n","    \n","    def save_to_word2vecfortmat(self,dict_data,path,vect_dimension=300):\n","        \"\"\"Sauvegarde le dictionnaire dans un format word2vec dans un fichier texte\"\"\"\n","        i=0\n","        list_embeddings=[]\n","        \n","        list_embeddings.insert(0,str(i)+\" \"+str(vect_dimension))\n","        final_write=\"\\n\".join(list_embeddings)\n","        with open(path,\"w\") as f:\n","            for w in dict_data:\n","                f.writelines(\" \".join([w,\" \".join(dict_data[w].astype(str))]))\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","pre=Prepocessing()\n","\n","class MyTranslator:\n","  translate_word={}\n","  pre=Prepocessing()\n","  def __init__(self):\n","    pass\n","  @classmethod\n","  def translate_sentence(cls,sentence:str):\n","    #on split la phrase en mot\n","    \n","    sentence=pre.clean_emoji(sentence)\n","    sentence=sentence.split()\n","    \n","    final_sentence=[]\n","    my_punct=string.punctuation+\"»«'´’…‘”“,¨¦´ʼʽʾʿˀˁ͵ͺ΄ʻ·\"\n","    for elt in sentence:\n","      if (elt==\"ˁ\"):\n","        final_sentence.append(\"ˀ\")\n","      elif elt in my_punct:\n","        final_sentence.append(elt)\n","      else:\n","        elt_2=cls.translate_word.get(elt,None)\n","        if (elt_2==None):\n","          final_sentence.append(elt)\n","        else:\n","          final_sentence.append(elt_2)\n","    \n","    return \" \".join(final_sentence)\n","\n","class MyTranslatorModel:\n","  def __init__(self,model_ibo,translator):\n","    self.model=model_ibo\n","    self.translator=translator\n","  \n","  def translate(self):\n","    pass"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dSVjhZZ_3Zbu"},"source":["pre=Prepocessing()\n","\n","french_sentences=pre.load_txt_sentences(\"/content/gdrive/MyDrive/absa_carmel/data/french_data/tripadvisor_commentt_fr.txt\")\n","all_fr_words={}\n","my_punct=string.punctuation+\"»«'´’…‘”“,¨¦´ʼʽʾʿˀˁ͵ͺ΄ʻ·\"\n","sentences_to_form_word_to_vect=[]\n","i=0\n","for elt in french_sentences:\n","  elt=elt.lower()\n","  elt=pre.replace_punctuaion(elt,my_punct)\n","  elt_2=pre.clean_emoji(elt)\n","  elt_s=elt_2.split()\n","  sentences_to_form_word_to_vect.append(elt_s)\n","  for elt2 in elt_s:\n","    if (elt2.strip()!=\"\"):\n","      all_fr_words[elt2]=True\n","print(len(all_fr_words))\n","word_list=list(all_fr_words)\n","word_to_translate=word_list\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G4vjwKbS5jhG"},"source":["from gensim.models import Word2Vec\n","\n","word_2vec_fr=Word2Vec(sentences_to_form_word_to_vect,min_count=1,iter=100,size=50)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b2q5s4qF7dvp"},"source":["word_2vec_fr.wv.save_word2vec_format(\"/content/gdrive/MyDrive/absa_carmel/data/word_representation/perso_embedding_fr_3.w2v\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3ihsBCUS9t66","executionInfo":{"status":"ok","timestamp":1607443488285,"user_tz":-60,"elapsed":137488,"user":{"displayName":"Franklin Dongmo nzoiyem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi7AvwmfyGzh2fAZxifqsrWYdRNaic02XVH0YZ4=s64","userId":"14051344547408147470"}},"outputId":"31021ae1-7290-4837-b832-fac13dad3a40"},"source":["new_fr_model={}\n","length_fr=0\n","for elt in model_fr.wv.vocab:\n","  #print(elt)\n","  new_fr_model[elt]=elt+\" \"+(\" \".join(model_fr[elt].astype(str)))\n","  length_fr+=1\n","\n","for elt in word_2vec_fr.wv.vocab:\n","  exist_gt=new_fr_model.get(elt,None)\n","  if (exist_gt==None):\n","    new_fr_model[elt]=elt+\" \"+(\" \".join(word_2vec_fr[elt].astype(str)))\n","    length_fr+=1\n","    \n","with open(\"/content/gdrive/MyDrive/absa_carmel/data/word_representation/cc.fr.50_2.vec\",\"w\") as f:\n","  first_line=str(length_fr)+\" 50\\n\"\n","  f.write(first_line)\n","  for elt in new_fr_model:\n","    f.write(new_fr_model[elt]+\"\\n\")\n","\n","\n","print(\"terminé....\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  # This is added back by InteractiveShellApp.init_path()\n"],"name":"stderr"},{"output_type":"stream","text":["terminé....\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4TdL-OvlAtzF","executionInfo":{"status":"ok","timestamp":1607442140872,"user_tz":-60,"elapsed":795,"user":{"displayName":"Franklin Dongmo nzoiyem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi7AvwmfyGzh2fAZxifqsrWYdRNaic02XVH0YZ4=s64","userId":"14051344547408147470"}},"outputId":"87ab3275-d705-4fd9-a4de-09a1e0a0b20e"},"source":["\n","  break"],"execution_count":null,"outputs":[{"output_type":"stream","text":["ma\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NdNVVLKeAq9B"},"source":["model_en=FastText.load_fasttext_format(\"/content/gdrive/MyDrive/absa_carmel/data/word_representation/cc.en.50.bin\")\n","model_fr=FastText.load_fasttext_format(\"/content/gdrive/MyDrive/absa_carmel/data/word_representation/cc.fr.50.bin\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EnaYpCym49Mg"},"source":["word_pairs=pre.load_object(\"/content/gdrive/MyDrive/absa_carmel/data/word_representation/train_words_pairs.bin\")\n","j=0\n","new_word_pairs=[]\n","for elt in word_pairs:\n","  if (new_fr_model.get(elt[0],None)!=None):\n","    new_word_pairs.append(elt)\n","new_word_pairs.append((\"dolé\",\"food\"))\n","new_word_pairs.append((\"doumba\",\"food\"))\n","new_word_pairs.append((\"ndolet\",\"food\"))\n","\n","pre.save_object(new_word_pairs,\"/content/gdrive/MyDrive/absa_carmel/data/word_representation/train_words_pairs.bin\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U7kCqUBNba5F","executionInfo":{"status":"ok","timestamp":1607449743397,"user_tz":-60,"elapsed":941,"user":{"displayName":"Franklin Dongmo nzoiyem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi7AvwmfyGzh2fAZxifqsrWYdRNaic02XVH0YZ4=s64","userId":"14051344547408147470"}},"outputId":"d186fe6d-4d7c-49f1-8eb4-d89999ab6084"},"source":["print(len(word_pairs))\n","print(len(new_word_pairs))\n","print(len(all_fr_words))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["8204\n","8204\n","9528\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AIbpmyEFs3kf"},"source":["\n","#print(model_fr[\"doumba\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ir4XN8ortgiM"},"source":["#creattion de la matrice de traduction\n","#model_en=KeyedVectors.load_word2vec_format(\"/content/gdrive/MyDrive/absa_carmel/data/word_representation/cc.en.50_2.vec\")\n","model_fr_n=KeyedVectors.load_word2vec_format(\"/content/gdrive/MyDrive/absa_carmel/data/word_representation/cc.fr.50_2.vec\")\n","#word_pairs=pre.load_object(\"/content/gdrive/MyDrive/absa_carmel/data/word_representation/train_words_pairs.bin\")\n","#word_pairs.append((\"dolé\",\"food\"))\n","#word_pairs.append((\"doumba\",\"food\"))\n","#word_pairs.append((\"ndolet\",\"food\"))\n","#translation_model=TranslationMatrix(model_fr,model_en,word_pairs=word_pairs)\n","#translation_model.train(word_pairs)\n","#translation_model.save(\"/content/gdrive/MyDrive/absa_carmel/model/translation_matrix/translation_matrix3.bin\")\n","#translation_model=TranslationMatrix.load(\"/content/gdrive/MyDrive/absa_carmel/model/translation_matrix/translation_matrix3.bin\")\n","test_punct=\"»«'´’…‘”“,¨¦´ʼʽʾʿˀˁ͵ͺ΄ʻ·\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p9YzQhSi-xeR"},"source":["for pu in string.punctuation+test_punct:\n","  print(pu)\n","  try:\n","    model_en[pu]\n","  except Exception as e:\n","    print(\"u\")\n","    try:\n","      model_fr_n[pu]\n","      print(\"***\",pu,\" **** in fr\")\n","    except Exception as e:\n","      print(\"*** \",pu,\" *** not in fr and en\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TGs4-9oCdydY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oFNZleOJefab"},"source":["#sentence=\"bonjour la famille je fais des expériences\"\n","#rans=MyTranslator()\n","#MyTranslator.translator=translation_model\n","#result=MyTranslator.translate_sentence(sentence)\n","#result=[]\n","#MyTranslator.translator=[]\n","#print(result)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p0stYQ3Sfk5p","executionInfo":{"status":"ok","timestamp":1607433302583,"user_tz":-60,"elapsed":57023,"user":{"displayName":"Franklin Dongmo nzoiyem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi7AvwmfyGzh2fAZxifqsrWYdRNaic02XVH0YZ4=s64","userId":"14051344547408147470"}},"outputId":"07bfc99e-3f96-4849-dafd-3c30fcdd41b8"},"source":["#maintenant nous allons traduire toutes les phrases francaises en anglais\n","french_sentences=pre.load_object(\"/content/gdrive/MyDrive/absa_carmel/data/french_data/all_sentences.bin\")\n","with open(\"/content/gdrive/MyDrive/absa_carmel/data/french_data/tripadvisor_commentt_fr.txt\",\"r\") as f:\n","  data=f.read()\n","  french_sentences=data.split()\n","\n","traduction_en=[]\n","all_fr_words={}\n","my_punct=string.punctuation+\"»«'´’…‘”“,¨¦´ʼʽʾʿˀˁ͵ͺ΄ʻ·\"\n","sentences_to_form_word_to_vect=[]\n","i=0\n","for elt in french_sentences:\n","  elt=elt.lower()\n","  elt=pre.replace_punctuaion(elt,my_punct)\n","  elt_2=pre.clean_emoji(elt)\n","  elt_s=elt_2.split()\n","  sentences_to_form_word_to_vect.append(elt_s)\n","  for elt2 in elt_s:\n","    if (elt2.strip()!=\"\"):\n","      all_fr_words[elt2]=True\n","print(len(all_fr_words))\n","word_list=list(all_fr_words)\n","word_to_translate=word_list\n","#word_to_translate.remove(\"doumba\")\n","#word_to_translate.remove(\"dolé\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["9528\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CLZh6iZMQ9AX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607449875301,"user_tz":-60,"elapsed":812,"user":{"displayName":"Franklin Dongmo nzoiyem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi7AvwmfyGzh2fAZxifqsrWYdRNaic02XVH0YZ4=s64","userId":"14051344547408147470"}},"outputId":"bed29d22-706d-43cc-ef66-bf97effd60f5"},"source":["for w in word_list:\n","  if \"‘\" in w:\n","    print(w)\n","#pre.save_object(word_list,\"/content/gdrive/MyDrive/absa_carmel/data/french_data/all_word_list_fr.bin\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["‘\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KK9konsiK5MN","executionInfo":{"status":"ok","timestamp":1607450269606,"user_tz":-60,"elapsed":1109,"user":{"displayName":"Franklin Dongmo nzoiyem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi7AvwmfyGzh2fAZxifqsrWYdRNaic02XVH0YZ4=s64","userId":"14051344547408147470"}},"outputId":"9dee0540-13b1-4d31-f351-82b66223f398"},"source":["word_to_translate=pre.load_object(\"/content/gdrive/MyDrive/absa_carmel/data/french_data/all_word_list_fr.bin\")\n","length=len(word_to_translate)\n","#word_traduction_fr_to_en=pre.load_object(\"/content/gdrive/MyDrive/absa_carmel/data/french_data/words_fr_to_en_2.bin\")\n","#print(len(word_traduction_fr_to_en))\n","\n","print(length)\n","print(word_to_translate[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["9528\n","ma\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d-46xVKoVEAL","executionInfo":{"elapsed":932,"status":"ok","timestamp":1606993405951,"user":{"displayName":"Franklin Dongmo nzoiyem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi7AvwmfyGzh2fAZxifqsrWYdRNaic02XVH0YZ4=s64","userId":"14051344547408147470"},"user_tz":-60},"outputId":"bd6c5091-be22-46f5-8812-5ce46265f3ed"},"source":["print(length)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["10362\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sm6JZ58v1zxg"},"source":["#print(gthyu)\n","#word_traduction_fr_to_en=pre.load_object(\"/content/gdrive/MyDrive/absa_carmel/data/cross_lingual/words_fr_to_en_2.bin\")\n","word_traduction_fr_to_en={}\n","for i in range(0,length,300):\n","  traductions=translation_model.translate(word_to_translate[i:i+300],topn=2)\n","  for elt in traductions:\n","    word_traduction_fr_to_en[elt]=traductions[elt][0]\n","  print(i)\n","  pre.save_object(word_traduction_fr_to_en,\"/content/gdrive/MyDrive/absa_carmel/data/cross_lingual/words_fr_to_en_3.bin\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xen93zBDchv6","executionInfo":{"elapsed":918,"status":"ok","timestamp":1606996359165,"user":{"displayName":"Franklin Dongmo nzoiyem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi7AvwmfyGzh2fAZxifqsrWYdRNaic02XVH0YZ4=s64","userId":"14051344547408147470"},"user_tz":-60},"outputId":"e1e9bc4e-4f9b-4b7c-ec28-bc997d24b9e7"},"source":["print(len(word_traduction_fr_to_en))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["10362\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hDcky_tpgpY7"},"source":["#print(i)\n","#word_list=list(word_traduction_fr_to_en)\n","#print(word_list[i])\n","#print(len(word_to_translate))\n","#print(len(word_traduction_fr_to_en))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X3QIgsuZ-q7H"},"source":["#pre.save_object(word_traduction_fr_to_en,\"/content/gdrive/MyDrive/absa_carmel/data/french_data/words_fr_to_en.bin\")\n","#print(i)\n","#print(word_to_translate[2399])\n","#print(len(word_traduction_fr_to_en))\n","#list_w=list(word_traduction_fr_to_en)\n","#print(list_w[3872])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j2DbgI44tYfd","executionInfo":{"elapsed":1199,"status":"ok","timestamp":1607168543078,"user":{"displayName":"Franklin Dongmo nzoiyem","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi7AvwmfyGzh2fAZxifqsrWYdRNaic02XVH0YZ4=s64","userId":"14051344547408147470"},"user_tz":-60},"outputId":"f2492d2f-2521-4dde-af72-b8af1c4b9e70"},"source":["print(list(range(2)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0, 1]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vzCKZfuQD6cm"},"source":["#nous allons traduit tous les mots  francais en anglais\n","#word_traduction_fr_to_en=pre.load_object(\"/content/gdrive/MyDrive/absa_carmel/data/french_data/words_fr_to_en.bin\")\n","#i=383\n","#for j in range(i,10364):\n","#  elt=gthyu[j]\n","#  if (elt.strip()!=\"\"):\n","#    trad=result=MyTranslator.translate_sentence(elt)\n","#    word_traduction_fr_to_en[elt]=trad\n","#pre.save_object(word_traduction_fr_to_en,\"/content/gdrive/MyDrive/absa_carmel/data/french_data/words_fr_to_en.bin\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R5PZAScbP8dN"},"source":["#pre.save_object(word_traduction_fr_to_en,\"/content/gdrive/MyDrive/absa_carmel/data/french_data/words_fr_to_en.bin\")\n","#print(len(word_traduction_fr_to_en))\n","#gthy=list(all_fr_words)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TKGT5y703YiP"},"source":["#print(word_to_translate)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m3pq_ZV2D2k5"},"source":["french_sentences=pre.load_txt_sentences(\"/content/gdrive/MyDrive/absa_carmel/data/french_data/tripadvisor_commentt_fr.txt\")\n","my_punct=string.punctuation+\"»«'´’…‘”“,¨¦´ʼʽʾʿˀˁ͵ͺ΄ʻ·\"\n","\n","i=0\n","for elt in french_sentences:\n","  elt=elt.lower()\n","  elt=pre.replace_punctuaion(elt,my_punct)\n","  elt=pre.clean_emoji(elt)\n","  french_sentences[i]=\" \".join(elt.split())\n","  i=i+1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jyWRd8gLD9Kv"},"source":["pre.save_object(french_sentences,\"/content/gdrive/MyDrive/absa_carmel/data/french_data/all_french_sentences.bin\")\n","french_sentences[0]\n","word_traduction_fr_to_en"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vvYHLnOkHHva"},"source":["MyTranslator.translate_word=word_traduction_fr_to_en\n","nsentences=[]\n","old_sentence=[]\n","sentence_trad=[]\n","for sentence in french_sentences:\n","  #sentence=sentence.lower()\n","  #no_emoji=pre.clean_emoji(sentence)\n","  new_sentence=MyTranslator.translate_sentence(sentence)\n","  nsentences.append(new_sentence)\n","  old_sentence.append(sentence)\n","\n","  sentence_trad.append(sentence+\"\\n\"+new_sentence)\n","\n","import json"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NjbtVKwfHu-A"},"source":["i=0\n","json_trad=[]\n","for elt in old_sentence:\n","  json_trad.append({\"original\":elt,\"traduction\":nsentences[i]})\n","  i=i+1\n","json_trad={\"sentences\":json_trad}\n","\n","with open(\"/content/gdrive/MyDrive/absa_carmel/data/traduction_en_fr_3.json\", \"w\",encoding='utf-8') as f:\n","  json.dump(json_trad,f,ensure_ascii=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nk3-370KHzjs"},"source":["#maintenant nous devons ecrit cela dans un fichier excel\n","#import pandas as pd\n","\n"],"execution_count":null,"outputs":[]}]}